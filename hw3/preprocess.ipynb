{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathlib.Path('../data/dict.txt').exists():\n",
    "    !curl -O https://github.com/ldkrsi/jieba-zh_TW/blob/master/jieba/dict.txt ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model cost 1.405 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.556 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.693 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed       1000 records\n",
      "Processed       2000 records\n",
      "Processed       3000 records\n",
      "Processed       4000 records\n",
      "Processed       5000 records\n",
      "Total: 5000\n",
      "CPU times: user 1.57 s, sys: 187 ms, total: 1.76 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from itertools import islice\n",
    "\n",
    "import jieba\n",
    "jieba.set_dictionary('../data/dict.txt')\n",
    "\n",
    "def extract_record(f, max_records=10000):\n",
    "    keys = [\n",
    "        '@url:', '@MainTextMD5:', '@UntagMD5:', '@SiteCode:', '@UrlCode:', '@title:', \n",
    "        '@Size:', '@keyword:', '@image_links:', '@Fetchtime:', '@post_time:', '@Ref:',\n",
    "        '@BodyMD5:', '@Lang:', '@IP:',\n",
    "    ]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line == '': # EOF\n",
    "            break\n",
    "\n",
    "        line = line.strip()\n",
    "        if line == '@': # record 的開始\n",
    "            record = dict()\n",
    "            continue\n",
    "        if line.startswith('@body:'): # 特判\n",
    "            time = f.readline() # discard timestamp\n",
    "            text = f.readline().strip()\n",
    "            record['body'] = line[6:] + time + text\n",
    "            yield record\n",
    "            continue\n",
    "        \n",
    "        for key in keys: # 取出要記錄的欄位\n",
    "            if line.startswith(key):\n",
    "                value = line[len(key):]\n",
    "                value = value if value != 'none' else None\n",
    "                record[key[1:-1].lower()] = value\n",
    "                break\n",
    "\n",
    "                \n",
    "def segmentize(arg):\n",
    "    idx, record = arg\n",
    "    record['body'] = ' '.join(jieba.cut(record['body'], cut_all=False))\n",
    "    out_path = out_dir / f'{idx:010d}.json'\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "    return idx\n",
    "\n",
    "\n",
    "raw = pathlib.Path('../data/ettoday').resolve()\n",
    "out_dir = pathlib.Path('../data/json/').resolve()\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(str(out_dir))\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 資料有混合編碼，開檔時記得指定 erros 的處理方法\n",
    "with open(raw, encoding='utf-8', errors='ignore') as f:\n",
    "    with mp.Pool(3) as p:\n",
    "        record_gen = enumerate(extract_record(f))\n",
    "        firstn_gen = islice(record_gen, 5_000)\n",
    "        result_gen = p.imap(segmentize, firstn_gen)\n",
    "        for idx in result_gen:\n",
    "            if (idx + 1) % 1_000 == 0:\n",
    "                print(f'Processed {idx+1:10d} records')\n",
    "        \n",
    "print('Total:', idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "# !cat ../data/json/0000000000.json\n",
    "# !echo \"\"\n",
    "# !cat ../data/json/0000000003.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308431\n",
      "['通過' '深刻' '表示' '也' '拿' '品德' '銀行' '人' '針對' '外銷']\n"
     ]
    }
   ],
   "source": [
    "dict_path = pathlib.Path('../data/dict.txt').resolve()\n",
    "with dict_path.open() as f:\n",
    "    queries = []\n",
    "    weights = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        queries.append(line[0])\n",
    "        weights.append(int(line[1]))\n",
    "\n",
    "print(len(queries))\n",
    "weights = np.float32(weights)\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "def get_queries(n):\n",
    "    return np.random.choice(queries, size=n, p=weights)\n",
    "\n",
    "queries = get_queries(100_000)\n",
    "np.save('../data/queries.npy', queries)\n",
    "print(queries[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 722 ms, sys: 130 ms, total: 852 ms\n",
      "Wall time: 857 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pathlib\n",
    "\n",
    "json_paths = sorted(pathlib.Path('../data/json').glob('*.json'))\n",
    "large_json = pathlib.Path('../data/single.json')\n",
    "with large_json.open('w') as fout:\n",
    "    fout.write('[')\n",
    "    for i, path in enumerate(json_paths):\n",
    "        if i != 0:\n",
    "            fout.write(',')\n",
    "        with path.open('r') as fin:\n",
    "            fout.write(fin.read())\n",
    "    fout.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
