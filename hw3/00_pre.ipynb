{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathlib.Path('../data/dict.txt').exists():\n",
    "    !curl -O https://github.com/ldkrsi/jieba-zh_TW/blob/master/jieba/dict.txt ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model cost 1.432 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.450 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.598 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed      20000 records\n",
      "Processed      40000 records\n",
      "Processed      60000 records\n",
      "Processed      80000 records\n",
      "Processed     100000 records\n",
      "Processed     120000 records\n",
      "Processed     140000 records\n",
      "Processed     160000 records\n",
      "Processed     180000 records\n",
      "Processed     200000 records\n",
      "Processed     220000 records\n",
      "Processed     240000 records\n",
      "Processed     260000 records\n",
      "Processed     280000 records\n",
      "Processed     300000 records\n",
      "Processed     320000 records\n",
      "Processed     340000 records\n",
      "Processed     360000 records\n",
      "Processed     380000 records\n",
      "Processed     400000 records\n",
      "Processed     420000 records\n",
      "Processed     440000 records\n",
      "Processed     460000 records\n",
      "Processed     480000 records\n",
      "Processed     500000 records\n",
      "Total: 500000\n",
      "CPU times: user 2min 28s, sys: 14.6 s, total: 2min 43s\n",
      "Wall time: 22min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from itertools import islice\n",
    "\n",
    "import jieba\n",
    "jieba.set_dictionary('../data/dict.txt')\n",
    "\n",
    "def extract_record(f):\n",
    "    keys = [\n",
    "        '@url:', '@MainTextMD5:', '@UntagMD5:', '@SiteCode:', '@UrlCode:', '@title:', \n",
    "        '@Size:', '@keyword:', '@image_links:', '@Fetchtime:', '@post_time:', '@Ref:',\n",
    "        '@BodyMD5:', '@Lang:', '@IP:',\n",
    "    ]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line == '': # EOF\n",
    "            break\n",
    "\n",
    "        line = line.strip()\n",
    "        if line == '@': # record 的開始\n",
    "            record = dict()\n",
    "            continue\n",
    "        if line.startswith('@body:'): # 特判\n",
    "            time = f.readline()\n",
    "            text = f.readline().strip()\n",
    "            record['body'] = line[6:] + time + text\n",
    "            yield record\n",
    "            continue\n",
    "        \n",
    "        for key in keys: # 取出要記錄的欄位\n",
    "            if line.startswith(key):\n",
    "                value = line[len(key):]\n",
    "                value = value if value != 'none' else None\n",
    "                record[key[1:-1].lower()] = value\n",
    "                break\n",
    "\n",
    "                \n",
    "def segmentize(arg):\n",
    "    idx, record = arg\n",
    "    record['body'] = ' '.join(jieba.cut(record['body'], cut_all=False))\n",
    "    out_path = out_dir / f'{idx:010d}.json'\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "    return idx\n",
    "\n",
    "\n",
    "raw = pathlib.Path('../data/ettoday').resolve()\n",
    "out_dir = pathlib.Path('../data/json/').resolve()\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(str(out_dir))\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 資料有混合編碼，開檔時記得指定 erros 的處理方法\n",
    "with open(raw, encoding='utf-8', errors='ignore') as f:\n",
    "    with mp.Pool(3) as p:\n",
    "        record_gen = enumerate(extract_record(f))\n",
    "        firstn_gen = islice(record_gen, 500_000)\n",
    "        result_gen = p.imap(segmentize, firstn_gen)\n",
    "        for idx in result_gen:\n",
    "            if (idx + 1) % 20_000 == 0:\n",
    "                print(f'Processed {idx+1:10d} records')\n",
    "        \n",
    "print('Total:', idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "# !cat ../data/json/0000000000.json\n",
    "# !echo \"\"\n",
    "# !cat ../data/json/0000000003.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308431\n",
      "['終歸' '局' '成績' '聽' '開始' '腕' '設置' '歷程' '女廁' '時尚']\n"
     ]
    }
   ],
   "source": [
    "dict_path = pathlib.Path('../data/dict.txt').resolve()\n",
    "with dict_path.open() as f:\n",
    "    queries = []\n",
    "    weights = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        queries.append(line[0])\n",
    "        weights.append(int(line[1]))\n",
    "\n",
    "print(len(queries))\n",
    "weights = np.float32(weights)\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "def get_queries(n):\n",
    "    return np.random.choice(queries, size=n, p=weights)\n",
    "\n",
    "queries = get_queries(100_000)\n",
    "np.save('../data/queries.npy', queries)\n",
    "print(queries[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
