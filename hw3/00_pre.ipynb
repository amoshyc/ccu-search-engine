{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pathlib.Path('../data/dict.txt').exists():\n",
    "    !curl -O https://github.com/ldkrsi/jieba-zh_TW/blob/master/jieba/dict.txt ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Building prefix dict from /home/amoshyc/workspace/ccu-search-engine/data/dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ube0cd633b915ed66f2168c08d0d21602.cache\n",
      "Loading model cost 1.432 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.450 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.598 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed      20000 records\n",
      "Processed      40000 records\n",
      "Processed      60000 records\n",
      "Processed      80000 records\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from itertools import islice\n",
    "\n",
    "import jieba\n",
    "jieba.set_dictionary('../data/dict.txt')\n",
    "\n",
    "def extract_record(f):\n",
    "    keys = [\n",
    "        '@url:', '@MainTextMD5:', '@UntagMD5:', '@SiteCode:', '@UrlCode:', '@title:', \n",
    "        '@Size:', '@keyword:', '@image_links:', '@Fetchtime:', '@post_time:', '@Ref:',\n",
    "        '@BodyMD5:', '@Lang:', '@IP:',\n",
    "    ]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line == '': # EOF\n",
    "            break\n",
    "\n",
    "        line = line.strip()\n",
    "        if line == '@': # record 的開始\n",
    "            record = dict()\n",
    "            continue\n",
    "        if line.startswith('@body:'): # 特判\n",
    "            time = f.readline() # discard timestamp\n",
    "            text = f.readline().strip()\n",
    "            record['body'] = line[6:] + time + text\n",
    "            yield record\n",
    "            continue\n",
    "        \n",
    "        for key in keys: # 取出要記錄的欄位\n",
    "            if line.startswith(key):\n",
    "                value = line[len(key):]\n",
    "                value = value if value != 'none' else None\n",
    "                record[key[1:-1].lower()] = value\n",
    "                break\n",
    "\n",
    "                \n",
    "def segmentize(arg):\n",
    "    idx, record = arg\n",
    "    record['body'] = ' '.join(jieba.cut(record['body'], cut_all=False))\n",
    "    out_path = out_dir / f'{idx:010d}.json'\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "    return idx\n",
    "\n",
    "\n",
    "raw = pathlib.Path('../data/ettoday').resolve()\n",
    "out_dir = pathlib.Path('../data/json/').resolve()\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(str(out_dir))\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 資料有混合編碼，開檔時記得指定 erros 的處理方法\n",
    "with open(raw, encoding='utf-8', errors='ignore') as f:\n",
    "    with mp.Pool(3) as p:\n",
    "        record_gen = enumerate(extract_record(f))\n",
    "        firstn_gen = islice(record_gen, 500_000)\n",
    "        result_gen = p.imap(segmentize, firstn_gen)\n",
    "        for idx in result_gen:\n",
    "            if (idx + 1) % 20_000 == 0:\n",
    "                print(f'Processed {idx+1:10d} records')\n",
    "        \n",
    "print('Total:', idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "# !cat ../data/json/0000000000.json\n",
    "# !echo \"\"\n",
    "# !cat ../data/json/0000000003.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = pathlib.Path('../data/dict.txt').resolve()\n",
    "with dict_path.open() as f:\n",
    "    queries = []\n",
    "    weights = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        queries.append(line[0])\n",
    "        weights.append(int(line[1]))\n",
    "\n",
    "print(len(queries))\n",
    "weights = np.float32(weights)\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "def get_queries(n):\n",
    "    return np.random.choice(queries, size=n, p=weights)\n",
    "\n",
    "queries = get_queries(100_000)\n",
    "np.save('../data/queries.npy', queries)\n",
    "print(queries[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
